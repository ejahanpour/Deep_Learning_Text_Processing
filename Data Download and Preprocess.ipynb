{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the required python files\n",
    "import os\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import itertools #to show the items in dictionary\n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First download the file from the source. This Udacity training is based on [Text8](http://mattmahoney.net/dc/textdata) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def download_text8(file_name, expected_bytes):\n",
    "    # Check the existing of the file and download if it does not exist\n",
    "    if not os.path.exists(file_name):\n",
    "        file_name, _ = urlretrieve(url+file_name, file_name)\n",
    "    stat_info = os.stat(file_name)\n",
    "    if stat_info.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % file_name)\n",
    "    else:\n",
    "        print(stat_info.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + file_name + '. Can you get to it with a browser?')\n",
    "    return file_name\n",
    "\n",
    "file_name = download_text8('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 17005207\n",
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst', 'the', 'term', 'is', 'still', 'used', 'in', 'a', 'pejorative', 'way', 'to', 'describe', 'any', 'act', 'that', 'used', 'violent', 'means', 'to', 'destroy', 'the', 'organization', 'of', 'society', 'it', 'has', 'also', 'been', 'taken', 'up', 'as', 'a', 'positive', 'label', 'by', 'self', 'defined', 'anarchists', 'the', 'word', 'anarchism', 'is', 'derived', 'from', 'the', 'greek', 'without', 'archons', 'ruler', 'chief', 'king', 'anarchism', 'as', 'a', 'political', 'philosophy', 'is', 'the', 'belief', 'that', 'rulers', 'are', 'unnecessary', 'and', 'should', 'be', 'abolished', 'although', 'there', 'are', 'differing']\n"
     ]
    }
   ],
   "source": [
    "def read_data(file_name):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n",
    "    with zipfile.ZipFile(file_name) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "  \n",
    "words = read_data(file_name)\n",
    "print('Data size %d' % len(words))\n",
    "# Sample of the sentence splitted as words\n",
    "print(words[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the dictionary and replace rare words with UNK token. using collections library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Enter the most common words as vocabualry\n",
    "vocabulary_size = 50000\n",
    "\n",
    "def build_dataset(words):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary) # get the length of current dictionary as the index of the word\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count = unk_count + 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "del words  # Hint to reduce memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample data [5240, 3084, 12, 6, 195, 2, 3135, 46, 59, 156]\n",
      "sample dictionary items: \n",
      "\n",
      "toole 21274\n",
      "dollars 3645\n",
      "factorial 15110\n",
      "synthesized 9137\n",
      "uprooted 39723\n"
     ]
    }
   ],
   "source": [
    "# Get some information on the data counts and dictionaries\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10])\n",
    "print('sample dictionary items: \\n')\n",
    "dicts= itertools.islice(dictionary.items(),0,5)\n",
    "for key, values in dicts: \n",
    "    print(key, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate a training batch for the skip-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first']\n",
      "\n",
      " with num_skip = 2 and skip_window=1:\n",
      "    batch: ['originated', 'originated', 'as', 'as', 'a', 'a', 'term', 'term']\n",
      "    labels: ['anarchism', 'as', 'originated', 'a', 'as', 'term', 'of', 'a']\n",
      "\n",
      " with num_skip = 4 and skip_window=2:\n",
      "    batch: ['as', 'as', 'as', 'as', 'a', 'a', 'a', 'a']\n",
      "    labels: ['originated', 'a', 'anarchism', 'term', 'term', 'of', 'originated', 'as']\n"
     ]
    }
   ],
   "source": [
    "# batch_size determines how many words we need in the training step\n",
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size,1), dtype=np.int32)\n",
    "    span = 2*skip_window +1 #[skip_window target skip_window]\n",
    "    buffer =collections.deque(maxlen=span) #list like container with fast appends and pops on either end\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "        \n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window # target label at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):  # positions the words around the target in the \"label\" array\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span -1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i*num_skips + j] = buffer[skip_window]\n",
    "            labels[i*num_skips+j,0] = buffer[target]\n",
    "        buffer.append(data[data_index]) # the append pops up the first buffer element and renew the target \n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "print('data: ', [reverse_dictionary[di] for di in data[:8]])\n",
    "\n",
    "for num_skips, skip_window in [(2,1), (4,2)]:\n",
    "    data_index = 0\n",
    "    batch, labels = generate_batch(batch_size=8, num_skips=num_skips, skip_window=skip_window)\n",
    "    print('\\n with num_skip = %d and skip_window=%d:' % (num_skips, skip_window))\n",
    "    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
    "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is time to convert words to vectors and train the model using skip-gram model. [This blog](https://iksinc.wordpress.com/tag/skip-gram-model/) provided a good explanation for word2vec algorithms especially Skip gram and bagging word algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128  # for training the model\n",
    "embedding_size = 128 # dimension of embedding vector\n",
    "# we will start from the second word in the document and consider one word to the right\n",
    "# and one word to the left of the central target word\n",
    "skip_window = 1 # How many words to consider left and right of the central word\n",
    "num_skips = 2 # How many times to reuse an input to generate a label\n",
    "# We pick a random validation set to sample nearest neighbors. here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. \n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "valid_window = 100 # Only pick samples in the head of the distribution.\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "num_sampled = 64 # Number of negative examples to sample.\n",
    "\n",
    "graph_skip = tf.Graph()\n",
    "\n",
    "with graph_skip.as_default(), tf.device('/cpu:0'):\n",
    "    \n",
    "    #input data\n",
    "    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size,1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    #variable\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    #Model\n",
    "    #look up embeddings for inputs\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed, \n",
    "                                   labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n",
    "    # Optimizer.\n",
    "    # Note: The optimizer will optimize the softmax_weights AND the embeddings.\n",
    "    # This is because the embeddings are defined as a variable quantity and the\n",
    "    # optimizer's `minimize` method will by default modify all variable quantities \n",
    "    # that contribute to the tensor it is passed.\n",
    "    # See docs on `tf.train.Optimizer.minimize()` for more details.\n",
    "\n",
    "    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "    # Compute the similarity between minibatch examples and all embeddings.\n",
    "    # We use the cosine distance:\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized!!\n",
      "average loss is at step 0: 1.000000\n",
      "Nearest to called: eudoxia cmd liturgies cheat unlike smack starry intercal\n",
      "Nearest to used: evading broaden wien cheeks exporter impinging naismith winston\n",
      "Nearest to on: damascus trintignant ordinals valid audiences avars gender vilayet\n",
      "Nearest to five: despatch miner spoofed irregulars appeasement cheaply cindy navin\n",
      "Nearest to also: cracking deals potter exclusively va insect webcams roman\n",
      "Nearest to UNK: champ smf deities ziibi raisins icmp measures filler\n",
      "Nearest to its: constipation maguire centurion magnon zagreb domes forefathers peep\n",
      "Nearest to have: inexpensive abode overlooked lecoq gorillaz granted machinima eurasian\n",
      "Nearest to may: deceiver tricked seek kaolinite sotho armaments zamboanga hs\n",
      "Nearest to he: fleshed tab neko partitions transmissible amalric royalist ppg\n",
      "Nearest to up: sadducees bidirectional melvin valleys hydrochloric era tibetan att\n",
      "Nearest to war: grimes fared magazine promiscuous milano neopagans micrometer mcconnell\n",
      "Nearest to eight: fips foobar wonderful abode rosser kanpur audacity vindication\n",
      "Nearest to however: smells aquaculture symbolising deleted escherichia intuitionistic fundamentals inhabitant\n",
      "Nearest to seven: customary letterman stringent schools creatively karel recompense middletown\n",
      "Nearest to can: irn sensors dawning lincoln midian wootz nbs candidates\n",
      "average loss is at step 2000: 1.000000\n",
      "average loss is at step 4000: 1.000000\n",
      "average loss is at step 6000: 1.000000\n",
      "average loss is at step 8000: 1.000000\n",
      "average loss is at step 10000: 1.000000\n",
      "Nearest to called: eudoxia varepsilon mommu multistage usenet unlike fernando lump\n",
      "Nearest to used: physics gatekeeper hautes automaker propanol magnetite militarily known\n",
      "Nearest to on: in at headroom tolbert by into and from\n",
      "Nearest to five: eight six seven three four nine two zero\n",
      "Nearest to also: which it utilising campos that sometimes cracking had\n",
      "Nearest to UNK: born acknowledgement jeho tardis congresses reticent officer authors\n",
      "Nearest to its: the their his fairies declares muscles roulette a\n",
      "Nearest to have: had be has were ethnomusicology are decapitate refers\n",
      "Nearest to may: would can tricked lengthy melchior chipset armaments kaolinite\n",
      "Nearest to he: it she they who venn agassiz there gathers\n",
      "Nearest to up: sadducees melvin tibetan out colitis poultry mont era\n",
      "Nearest to war: magazine grimes steve amerindian once promiscuous impotence fared\n",
      "Nearest to eight: nine six seven four five three zero two\n",
      "Nearest to however: constitutions seton symbolising chili since loon seperate smells\n",
      "Nearest to seven: six eight nine five zero four three two\n",
      "Nearest to can: might would may smell fiesole rgermeister could should\n",
      "average loss is at step 12000: 1.000000\n",
      "average loss is at step 14000: 1.000000\n",
      "average loss is at step 16000: 1.000000\n",
      "average loss is at step 18000: 1.000000\n",
      "average loss is at step 20000: 1.000000\n",
      "Nearest to called: eudoxia ropes unlike lump varepsilon com usenet innodb\n",
      "Nearest to used: known militarily able physics situated magnetite propanol performed\n",
      "Nearest to on: in at upon subpixels headroom eigenstates immunodeficiency supporting\n",
      "Nearest to five: eight four six seven three two zero nine\n",
      "Nearest to also: which now not who there sometimes usually campos\n",
      "Nearest to UNK: tardis n filmfour h ethiopian physicist intestines behaviour\n",
      "Nearest to its: their his the her fairies our titanic aise\n",
      "Nearest to have: had has be ethnomusicology were are refers take\n",
      "Nearest to may: can would must could should will lengthy might\n",
      "Nearest to he: it she who they there fragmentary gathers which\n",
      "Nearest to up: sadducees out melvin tibetan brewery callisthenes salut countrymen\n",
      "Nearest to war: grimes amerindian magazine promiscuous fared neopagans third latinus\n",
      "Nearest to eight: nine six seven four five three zero two\n",
      "Nearest to however: but since chili constitutions that seton growths sino\n",
      "Nearest to seven: eight four six five nine three two zero\n",
      "Nearest to can: would may could should might will must cannot\n",
      "average loss is at step 22000: 1.000000\n",
      "average loss is at step 24000: 1.000000\n",
      "average loss is at step 26000: 1.000000\n",
      "average loss is at step 28000: 1.000000\n",
      "average loss is at step 30000: 1.000000\n",
      "Nearest to called: ropes com unlike eudoxia addams lump aloe varepsilon\n",
      "Nearest to used: known able referred militarily performed physics described given\n",
      "Nearest to on: in upon headroom from at under immunodeficiency dispossessed\n",
      "Nearest to five: four seven six eight three nine zero two\n",
      "Nearest to also: which now it who livres sometimes campos still\n",
      "Nearest to UNK: elect filmfour congresses de dagger et populate comedians\n",
      "Nearest to its: their his the her monoxide aise nuke our\n",
      "Nearest to have: had has were be are ethnomusicology having do\n",
      "Nearest to may: can would must will could should might cannot\n",
      "Nearest to he: she it they who there fragmentary agassiz we\n",
      "Nearest to up: out sadducees back melvin salut mont him tibetan\n",
      "Nearest to war: amerindian grimes fared neopagans last aground storyteller hesiod\n",
      "Nearest to eight: nine six seven four five three zero two\n",
      "Nearest to however: but although since that though and constitutions like\n",
      "Nearest to seven: eight six four five nine three zero two\n",
      "Nearest to can: may could would will should must might cannot\n",
      "average loss is at step 32000: 1.000000\n",
      "average loss is at step 34000: 1.000000\n",
      "average loss is at step 36000: 1.000000\n",
      "average loss is at step 38000: 1.000000\n",
      "average loss is at step 40000: 1.000000\n",
      "Nearest to called: com shooter innodb addams ropes perthshire named dipoles\n",
      "Nearest to used: known able found described referred given available considered\n",
      "Nearest to on: upon congregationalists ruppert in during at immunodeficiency about\n",
      "Nearest to five: six seven four three eight zero nine two\n",
      "Nearest to also: which now still usually often sometimes who that\n",
      "Nearest to UNK: b louis unconditional and gotta pfeiffer lobbyist r\n",
      "Nearest to its: their his the our her fulfilling greenhouses aise\n",
      "Nearest to have: had has were be are ethnomusicology saab judas\n",
      "Nearest to may: can would will must could should cannot might\n",
      "Nearest to he: she it they who there i fragmentary agassiz\n",
      "Nearest to up: out back them sadducees him tibetan mont salut\n",
      "Nearest to war: amerindian grimes neopagans last storyteller narrowest mythology promiscuous\n",
      "Nearest to eight: nine seven six four five three zero two\n",
      "Nearest to however: but although though growths while since that indivisible\n",
      "Nearest to seven: eight six nine five four three zero two\n",
      "Nearest to can: may could will would must should might cannot\n",
      "average loss is at step 42000: 1.000000\n",
      "average loss is at step 44000: 1.000000\n",
      "average loss is at step 46000: 1.000000\n",
      "average loss is at step 48000: 1.000000\n",
      "average loss is at step 50000: 1.000000\n",
      "Nearest to called: named com shooter ropes known addams dehydroepiandrosterone aloe\n",
      "Nearest to used: known referred described available given able written found\n",
      "Nearest to on: upon in through using at into feldman immunodeficiency\n",
      "Nearest to five: four six eight seven nine three zero two\n",
      "Nearest to also: now which still often sometimes who usually then\n",
      "Nearest to UNK: schneider la herc del von worships reuven letterman\n",
      "Nearest to its: their his the her our miniaturized agnostics maclachlan\n",
      "Nearest to have: had has were are be ethnomusicology refer who\n",
      "Nearest to may: can would will must could should might cannot\n",
      "Nearest to he: she it they who there this agassiz fragmentary\n",
      "Nearest to up: out back them him sadducees off tibetan cena\n",
      "Nearest to war: amerindian grimes storyteller narrowest deuterocanonical neopagans promiscuous ebu\n",
      "Nearest to eight: six nine seven four five three zero two\n",
      "Nearest to however: but although though that while since when growths\n",
      "Nearest to seven: eight six nine four three five zero two\n",
      "Nearest to can: could may will would must should might cannot\n",
      "average loss is at step 52000: 1.000000\n",
      "average loss is at step 54000: 1.000000\n",
      "average loss is at step 56000: 1.000000\n",
      "average loss is at step 58000: 1.000000\n",
      "average loss is at step 60000: 1.000000\n",
      "Nearest to called: named used com known addams template shooter ropes\n",
      "Nearest to used: known described referred available seen found defined performed\n",
      "Nearest to on: upon in using congregationalists through insightful within valid\n",
      "Nearest to five: four six eight seven three nine zero two\n",
      "Nearest to also: now often still sometimes usually there generally solidification\n",
      "Nearest to UNK: lingua shiloh commodore smoker freleng landmarks discussed rev\n",
      "Nearest to its: their his her the your practical aise nuke\n",
      "Nearest to have: had has are were be ethnomusicology having judas\n",
      "Nearest to may: can would will could must should might cannot\n",
      "Nearest to he: she it they who there never agassiz converging\n",
      "Nearest to up: out back off him them down sadducees evaluate\n",
      "Nearest to war: amerindian deuterocanonical storyteller simons grimes narrowest outfits hui\n",
      "Nearest to eight: six nine seven four five three zero one\n",
      "Nearest to however: but although though since that which when and\n",
      "Nearest to seven: eight four six five nine three zero one\n",
      "Nearest to can: may could will would must should might cannot\n",
      "average loss is at step 62000: 1.000000\n",
      "average loss is at step 64000: 1.000000\n",
      "average loss is at step 66000: 1.000000\n",
      "average loss is at step 68000: 1.000000\n",
      "average loss is at step 70000: 1.000000\n",
      "Nearest to called: named known seniority considered com used addams template\n",
      "Nearest to used: known described seen found available referred regarded written\n",
      "Nearest to on: upon through in subpixels within congregationalists at about\n",
      "Nearest to five: four six seven three eight nine zero two\n",
      "Nearest to also: now often still which sometimes usually traditionally generally\n",
      "Nearest to UNK: diatoms poked rapper herc latterly fortified sufferers h\n",
      "Nearest to its: their his her the our greenhouses webpages kiesinger\n",
      "Nearest to have: had has were are be having ethnomusicology judas\n",
      "Nearest to may: can could would will should must might cannot\n",
      "Nearest to he: she it they there who agassiz never we\n",
      "Nearest to up: out off back them down evaluate sadducees him\n",
      "Nearest to war: amerindian storyteller deuterocanonical simons narrowest promiscuous outfits grimes\n",
      "Nearest to eight: nine six seven four five zero three two\n",
      "Nearest to however: but although though that while where which when\n",
      "Nearest to seven: six eight four nine five three zero two\n",
      "Nearest to can: may could will would must should might cannot\n",
      "average loss is at step 72000: 1.000000\n",
      "average loss is at step 74000: 1.000000\n",
      "average loss is at step 76000: 1.000000\n",
      "average loss is at step 78000: 1.000000\n",
      "average loss is at step 80000: 1.000000\n",
      "Nearest to called: named known considered used seniority com designed addams\n",
      "Nearest to used: known referred described seen found available written regarded\n",
      "Nearest to on: upon in through subpixels at supporting mouvement tableland\n",
      "Nearest to five: six four seven eight three nine zero two\n",
      "Nearest to also: now often still sometimes which below actually usually\n",
      "Nearest to UNK: dr rev ha jones lingua perry interjections beekeepers\n",
      "Nearest to its: their his her the your our miniaturized agnostics\n",
      "Nearest to have: had has were are be having ethnomusicology include\n",
      "Nearest to may: can could would must will should might cannot\n",
      "Nearest to he: she it they there who we never agassiz\n",
      "Nearest to up: out off them back down him eureka evaluate\n",
      "Nearest to war: amerindian storyteller deuterocanonical wars grimes simons outfits alfonso\n",
      "Nearest to eight: nine seven six five four three zero two\n",
      "Nearest to however: although but though that while since they where\n",
      "Nearest to seven: eight five six nine four three zero one\n",
      "Nearest to can: could may will would must should might cannot\n",
      "average loss is at step 82000: 1.000000\n",
      "average loss is at step 84000: 1.000000\n",
      "average loss is at step 86000: 1.000000\n",
      "average loss is at step 88000: 1.000000\n",
      "average loss is at step 90000: 1.000000\n",
      "Nearest to called: named known used considered referred template mandan seniority\n",
      "Nearest to used: known referred seen described regarded found considered useful\n",
      "Nearest to on: upon subpixels under in at through against idealised\n",
      "Nearest to five: seven eight six four three nine zero two\n",
      "Nearest to also: now often still which sometimes actually usually who\n",
      "Nearest to UNK: milli ha perry estonian tanaka neon rourke owen\n",
      "Nearest to its: their his her the your miniaturized our agnostics\n",
      "Nearest to have: had has were having be ethnomusicology are bump\n",
      "Nearest to may: can could would must should might will cannot\n",
      "Nearest to he: she it they there who originally later waylon\n",
      "Nearest to up: out off back them him down together away\n",
      "Nearest to war: amerindian storyteller battle deuterocanonical dart ongoing importance wars\n",
      "Nearest to eight: seven five nine six four three zero two\n",
      "Nearest to however: but although though that while since especially and\n",
      "Nearest to seven: eight five nine four six three zero one\n",
      "Nearest to can: could may will should must would cannot might\n",
      "average loss is at step 92000: 1.000000\n",
      "average loss is at step 94000: 1.000000\n",
      "average loss is at step 96000: 1.000000\n",
      "average loss is at step 98000: 1.000000\n",
      "average loss is at step 100000: 1.000000\n",
      "Nearest to called: named known used template referred designed considered mandan\n",
      "Nearest to used: referred found seen known described useful available considered\n",
      "Nearest to on: upon through at subpixels in within against under\n",
      "Nearest to five: seven six eight four three nine zero two\n",
      "Nearest to also: now still often actually sometimes below which usually\n",
      "Nearest to UNK: letterman fullmetal khomeini genocidal scanline rev clerk here\n",
      "Nearest to its: their his the her our your whose miniaturized\n",
      "Nearest to have: had has were are having be include ethnomusicology\n",
      "Nearest to may: can could must should would might will cannot\n",
      "Nearest to he: she it they who there we never eventually\n",
      "Nearest to up: out off back them him down me together\n",
      "Nearest to war: amerindian storyteller battle wars deuterocanonical exile outfits phoenicia\n",
      "Nearest to eight: seven nine six five four three zero two\n",
      "Nearest to however: but although though that while especially there and\n",
      "Nearest to seven: eight five nine six four three zero two\n",
      "Nearest to can: could may would must will should cannot might\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph_skip) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized!!')\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):       \n",
    "        batch_data, batch_label = generate_batch(batch_size=batch_size, \n",
    "                                                 num_skips=num_skips, skip_window=skip_window)\n",
    "        feed_dict = {train_dataset: batch_data, train_labels: batch_label}\n",
    "        _, l = sess.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss +=1       \n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / 2000\n",
    "            # average loss is an estimate of loss over the last 2000 batches\n",
    "            print(\"average loss is at step %d: %f\" %(step, average_loss))\n",
    "            average_loss = 0\n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8 #number of nearest neighbors\n",
    "                nearest = (-sim[i,:]).argsort()[1:top_k+1]\n",
    "                log = \"Nearest to %s:\" %valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log = '%s %s' %(log, close_word)\n",
    "                print(log)\n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_points = 400\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
